{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5388dbf-3275-40ba-8f47-c83bba2fa53b",
   "metadata": {},
   "source": [
    "# Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca6951b5-c41b-467d-8387-01421004eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ccxt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm.auto import tqdm \n",
    "from xgboost import XGBClassifier\n",
    "import json \n",
    "import pandas_ta as ta\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split \n",
    "import zoneinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06bda3a2-e097-4779-b4dc-d2813ec7a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df = pd.read_feather('BTC_USDT-1d (1).feather')\n",
    "chart_df['date'] = pd.to_datetime(chart_df['date'])\n",
    "# Extract year, month, day, and time information\n",
    "chart_df['month'] = chart_df['date'].dt.month\n",
    "chart_df['day'] = chart_df['date'].dt.day\n",
    "chart_df.drop(columns={\"date\"}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "add464f7-61ba-4915-928b-9a8a7e01bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "close = chart_df[\"close\"].values \n",
    "\n",
    "targets = [] \n",
    "for i in range(len(close)-1): \n",
    "    diff = (close[i+1] - close[i]) / close[i] \n",
    "    if diff < 0:\n",
    "        targets.append(0) # short \n",
    "    else: \n",
    "        targets.append(1) # long \n",
    "        \n",
    "targets.append(None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eca1486e-e0fd-45ac-89bc-63c2ef0e0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df[\"targets\"] = targets\n",
    "\n",
    "chart_df.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27b6ae1c-0496-40cb-bb72-e75e9fd177b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "scaled_features = scaler.fit_transform(chart_df.drop('targets', axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43d1d0bb-c6d9-442c-b5ce-72022d11fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y, time_step=1): \n",
    "    Xs, ys = [], [] \n",
    "    for i in range(len(X) - time_steps): \n",
    "        v = X.iloc[i:(i + time_steps)].values \n",
    "        Xs.append(v) \n",
    "        ys.append(y.iloc[i + time_steps]) \n",
    "    return np.array(Xs), np.array(ys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecabfc40-e40b-4885-a7c3-5b79af10afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 14\n",
    "X, y = create_dataset(pd.DataFrame(scaled_features), chart_df['targets'], time_steps)\n",
    "\n",
    "train_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:train_size] \n",
    "y_train = y[:train_size] \n",
    "\n",
    "X_val = X[train_size:] \n",
    "y_val = y[train_size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e806909-c5df-4005-a6de-4553eb4abca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1828, 14, 7]),\n",
       " torch.Size([1828]),\n",
       " torch.Size([458, 14, 7]),\n",
       " torch.Size([458]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=int)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=int)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b654b282-0975-472f-b23c-f80a7055bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(X_val, y_val) \n",
    "val_sampler = SequentialSampler(val_data) \n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a11b54bd-f234-4ee7-9c7d-58cec06cfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Classifier\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_heads, dim_feedforward, output_size):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer for positional encoding\n",
    "        self.embedding = nn.Linear(input_size, input_size)\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        # Linear layer for final output\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Positional embedding\n",
    "        x = self.embedding(x)\n",
    "        # Passing through the transformer layers\n",
    "        x = self.transformer_encoder(x)\n",
    "        # Taking the output of the last time step\n",
    "        x = x[:, -1, :] \n",
    "        # Output layer\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9928ba7e-088a-4e59-b13c-b197f1949bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e46b1995-a629-42d2-8fb3-366f5982962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed867296d2204908b8edee1247393273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.7022328561749952, Val Loss: 0.6972587466239929, val accuracy: 0.48333333333333334\n",
      "Epoch 2: Train Loss: 0.6965796238389509, Val Loss: 0.694024900595347, val accuracy: 0.49083333333333334\n",
      "Epoch 3: Train Loss: 0.6980716997179491, Val Loss: 0.6938722411791484, val accuracy: 0.4845833333333333\n",
      "Epoch 4: Train Loss: 0.69346975560846, Val Loss: 0.6937342643737793, val accuracy: 0.49083333333333334\n",
      "Epoch 5: Train Loss: 0.6965873775811031, Val Loss: 0.6936309655507406, val accuracy: 0.47625\n",
      "Epoch 6: Train Loss: 0.6961075057243479, Val Loss: 0.693880005677541, val accuracy: 0.5304166666666666\n",
      "Epoch 7: Train Loss: 0.6928976527575789, Val Loss: 0.6934242010116577, val accuracy: 0.495\n",
      "Epoch 8: Train Loss: 0.6933132944435909, Val Loss: 0.6937866568565368, val accuracy: 0.4791666666666667\n",
      "Epoch 9: Train Loss: 0.6975081429399294, Val Loss: 0.6936341603597005, val accuracy: 0.46375\n",
      "Epoch 10: Train Loss: 0.6961903674849148, Val Loss: 0.6933500369389852, val accuracy: 0.5283333333333333\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") \n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 7  # As per your dataset\n",
    "num_layers = 3  # Number of Transformer layers\n",
    "num_heads = 1   # Number of heads in Multi-Head Attention\n",
    "dim_feedforward = 512  # Feedforward dimension\n",
    "output_size = 2  # Number of output classes\n",
    "\n",
    "# Model initialization\n",
    "model = TransformerClassifier(input_size, num_layers, num_heads, dim_feedforward, output_size)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = np.inf\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_x, b_y = [t.to(device) for t in batch]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_accuracy = 0 \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            b_x, b_y = [t.to(device) for t in batch]\n",
    "            output = model(b_x)\n",
    "            loss = loss_func(output, b_y)\n",
    "            total_val_loss += loss.item()\n",
    "            val_accuracy += flat_accuracy(output.detach().cpu().numpy(), b_y.detach().cpu().numpy()) \n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    avg_val_accuracy = val_accuracy / len(val_dataloader) \n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_transformer_encoder.pt\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}, val accuracy: {avg_val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bc400c7-59ce-46f2-a894-e6fb11c62618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformer_minmax_scaler.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save scaler \n",
    "import joblib \n",
    "joblib.dump(scaler, 'Transformer_minmax_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5781a-a412-40b1-b75c-2987631cecb9",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6f6fa95-b8dd-4e3e-a0ff-a1bcb4c6c7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run inference on CPU \n",
    "input_size = 7  # As per your dataset\n",
    "num_layers = 3  # Number of Transformer layers\n",
    "num_heads = 1   # Number of heads in Multi-Head Attention\n",
    "dim_feedforward = 512  # Feedforward dimension\n",
    "output_size = 2  # Number of output classes\n",
    "\n",
    "Transformer_test = TransformerClassifier(input_size, num_layers, num_heads, dim_feedforward, output_size)  \n",
    "checkpoint = torch.load(\"best_transformer_encoder.pt\") \n",
    "Transformer_test.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2154af9b-c0c9-4d20-8cff-3c81c94053e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "test_scaler = joblib.load(\"Transformer_minmax_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8209ffb5-ab4f-4bc8-b27f-324668d83cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Convert timestamps to datetime objects in Korean timezone\n",
    "    korean_timezone = zoneinfo.ZoneInfo(\"Asia/Seoul\")\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)\n",
    "    df['datetime'] = df['datetime'].dt.tz_convert(korean_timezone)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14d6220e-429f-430d-9b24-a1fc64fafee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitget = ccxt.bitget()\n",
    "ohlcv = bitget.fetch_ohlcv(\"BTC/USDT:USDT\", \"1d\")\n",
    "chart_df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "chart_df = preprocess(chart_df)\n",
    "days, months = [], []\n",
    "for dt in chart_df[\"datetime\"]:\n",
    "    dtobj = pd.to_datetime(dt)\n",
    "    day = dtobj.day\n",
    "    month = dtobj.month\n",
    "    days.append(day)\n",
    "    months.append(month)\n",
    "chart_df[\"month\"] = months\n",
    "chart_df[\"day\"] = days\n",
    "chart_df.drop(columns={\"timestamp\", \"datetime\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f836a2f-d957-4db9-b645-6378296b33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_features = test_scaler.transform(chart_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee1ec8b3-4fec-4e9a-b499-6ccc41bb4198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long prob: 0.47848591208457947%\n",
      "short prob: 0.5215141177177429%\n"
     ]
    }
   ],
   "source": [
    "test_input = test_scaled_features[-15:-1]  \n",
    "test_input = test_input.reshape((-1, 14, 7)) \n",
    "test_input = torch.tensor(test_input, dtype=torch.float32)\n",
    "\n",
    "Transformer_test.eval() \n",
    "with torch.no_grad():\n",
    "    output = Transformer_test(test_input) \n",
    "    probs = nn.Softmax(dim=-1)(output)[0] \n",
    "    print(f\"long prob: {probs[1]}%\")  \n",
    "    print(f\"short prob: {probs[0]}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b283e61b-662f-4e18-a3de-00215aee8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference function \n",
    "def infer_Transformer(): \n",
    "    input_size = 7  # As per your dataset\n",
    "    num_layers = 3  # Number of Transformer layers\n",
    "    num_heads = 1   # Number of heads in Multi-Head Attention\n",
    "    dim_feedforward = 512  # Feedforward dimension\n",
    "    output_size = 2  # Number of output classes\n",
    "\n",
    "    Transformer_test = TransformerClassifier(input_size, num_layers, num_heads, dim_feedforward, output_size)  \n",
    "    checkpoint = torch.load(\"best_transformer_encoder.pt\") \n",
    "    Transformer_test.load_state_dict(checkpoint)\n",
    "    \n",
    "    test_scaler = joblib.load(\"Transformer_minmax_scaler.pkl\")\n",
    "    \n",
    "    bitget = ccxt.bitget()\n",
    "    ohlcv = bitget.fetch_ohlcv(\"BTC/USDT:USDT\", \"1d\")\n",
    "    chart_df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    chart_df = preprocess(chart_df)\n",
    "    days, months = [], []\n",
    "    for dt in chart_df[\"datetime\"]:\n",
    "        dtobj = pd.to_datetime(dt)\n",
    "        day = dtobj.day\n",
    "        month = dtobj.month\n",
    "        days.append(day)\n",
    "        months.append(month)\n",
    "    chart_df[\"month\"] = months\n",
    "    chart_df[\"day\"] = days\n",
    "    chart_df.drop(columns={\"timestamp\", \"datetime\"}, inplace=True)\n",
    "    \n",
    "    test_input = test_scaled_features[-15:-1]  \n",
    "    test_input = test_input.reshape((-1, 14, 7)) \n",
    "    test_input = torch.tensor(test_input, dtype=torch.float32)\n",
    "\n",
    "    Transformer_test.eval() \n",
    "    with torch.no_grad():\n",
    "        output = Transformer_test(test_input) \n",
    "        probs = nn.Softmax(dim=-1)(output)[0] \n",
    "        probs = probs.detach().cpu().numpy() \n",
    "\n",
    "    Transformer_long_prob = round(probs[1] * 100, 2)\n",
    "    Transformer_short_prob = round(probs[0] * 100, 2)\n",
    "    return Transformer_long_prob, Transformer_short_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cf56b-27d8-48d5-8108-4590d9332e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
